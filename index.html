<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MB0YKTRBHZ"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-MB0YKTRBHZ');
    </script>
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>ibl-nemo</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="icon" type="image/png" href="./index_files/icon.png">
    <link rel="stylesheet" href="./index_files/bootstrap.min.css">
    <link rel="stylesheet" href="./index_files/font-awesome.min.css">
    <link rel="stylesheet" href="./index_files/codemirror.min.css">
    <link rel="stylesheet" href="./index_files/app.css">
    <link rel="stylesheet" href="./index_files/bootstrap.min(1).css">

    <script type="text/javascript" async="" src="./index_files/analytics.js"></script>
    <script type="text/javascript" async="" src="./index_files/analytics(1).js"></script>
    <script async="" src="./index_files/js"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-110862391-3');
    </script>

    <script src="./index_files/jquery.min.js"></script>
    <script src="./index_files/bootstrap.min.js"></script>
    <script src="./index_files/codemirror.min.js"></script>
    <script src="./index_files/clipboard.min.js"></script>

    <script src="./index_files/app.js"></script>
    <style>
        h1 {
            margin-bottom: 0px !important;
            padding-bottom: 0px !important;
        }
    </style>
</head>      
    
<body data-gr-c-s-loaded="true">
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                In vivo cell-type and brain region classification via multimodal contrastive learning
            <br /><br />
            <!-- <small>
                July 2024
            </small> -->
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="mailto:hy2562@columbia.edu">
                            Han Yu
                        </a><sup>1</sup>
                    </li>
                    <li>
                        <a href="mailto:hanruilyu2029@northwestern.edu">
                          Hanrui Lyu
                        </a><sup>2</sup>
                    </li>                    
                   <li>
                        <a href="mailto:yx2740@columbia.edu">
                            Ethan Yixun Xu
                        </a><sup>1</sup>
                    </li>
                    <li>
                        <a href="mailto:ciw2107@columbia.edu">
                            Charlie Windolf
                        </a><sup>1</sup>
                    </li> 
                    <li>
                        <a href="mailto:kenjilee@bu.edu">
                            Eric Kenji Lee
                        </a><sup>3</sup>
                    </li> 
                    <li>
                        <a href="mailto:yfan7809@gmail.com">
                            Fan Yang
                        </a><sup>4</sup>
                    </li> 
                    <li>
                        <a href="mailto:andrew.shelton1312@gmail.com">
                            Andrew M. Shelton
                        </a><sup>5</sup>
                    </li> 
                    <li>
                        <a href="https://alleninstitute.org/person/shawn-olsen/">
                            Shawn Olsen
                        </a><sup>5</sup>
                    </li> 
                    <li>
                        <a href="mailto:smanavi.ctp@gmail.com">
                            Sahar Minavi
                        </a><sup>5</sup>
                    </li> 
                    <li>
                        <a href="mailto:olivier.winter@internationalbrainlab.org">
                            Olivier Winter
                        </a><sup>6</sup>
                    </li> 
                    <li>
                        <a href="https://www.internationalbrainlab.com/">
                            The International Brain Laboratory
                        </a>
                    </li>   
                    <li>
                        <a href="https://dyerlab.gatech.edu/">
                            Eva L. Dyer
                        </a><sup>7</sup>
                    </li> 
                    <li>
                        <a href="https://sites.bu.edu/chandlab/">
                            Chandramouli Chandrasekaran
                        </a><sup>3</sup>
                    </li> 
                    <li>
                        <a href="https://www.nicksteinmetz.com/">
                            Nicholas A. Steinmetz
                        </a><sup>8</sup>
                    </li> 
                    <li>
                        <a href="http://www.stat.columbia.edu/~liam/">
                            Liam Paninski
                        </a><sup>1</sup>
                    </li>
                    <li>
                        <a href="https://colehurwitz.github.io/">
                            Cole Hurwitz
                        </a><sup>1</sup>
                    </li>                       
                </ul>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <sup>1</sup>Columbia University
                    </li>
                    <li>
                        <sup>2</sup>Northwestern University
                    </li>
                    <li>
                        <sup>3</sup>Boston University
                    </li>
                    <li>
                        <sup>4</sup>University of College London
                    </li>
                    <br>
                    <li>
                        <sup>5</sup>Allen Institute
                    </li>
                    <li>
                        <sup>6</sup>Champalimaud Foundation
                    </li>
                    <li>
                        <sup>7</sup>Georgia Institute of Technology
                    </li>
                    <li>
                        <sup>8</sup>University of Washington
                    </li>
                    <br>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://openreview.net/forum?id=10JOlFIPjt" target='_blank'>
                        <img src="index_files/images/nemo_paper_preview.png" height="80px"><br>
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/Haansololfp/NEMO">
                        <img src="./index_files/images/github_pad.png" height="80px"><br>
                            <h4><strong>Code</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="#BibTeX">
                        <img src="./index_files/images/bibtex.jpg" height="80px"><br>
                            <h4><strong>BibTeX</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
                <img src="./index_files/images/NEMO_overview.png" class="img-responsive" alt="overview"><br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Current electrophysiological approaches can track the activity of many neurons, yet it is usually unknown which cell-types or brain areas are being recorded without further molecular or histological analysis. Developing accurate and scalable algorithms for identifying the cell-type and brain region of recorded neurons is thus crucial for improving our understanding of neural computation. In this work, we develop a multimodal contrastive learning approach for neural data that can be fine-tuned for different downstream tasks, including inference of cell-type and brain location. We utilize multimodal contrastive learning to jointly embed the activity autocorrelations and extracellular waveforms of individual neurons. We demonstrate that our embedding approach, Neuronal Embeddings via MultimOdal Contrastive Learning (NEMO), paired with supervised fine-tuning, achieves state-of-the-art cell-type classification for two opto-tagged datasets and brain region classification for the public International Brain Laboratory Brain-wide Map dataset. Our method represents a promising step towards accurate cell-type and brain region classification from electrophysiological recordings. Code is availabe at https://github.com/Haansololfp/NEMO.
                </p>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Highlights
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            A multimodal contrastive learning method for electrophysiological data, Neuronal Embeddings via MultimOdal Contrastive Learning (NEMO).
                        </li>
                        <li>
                            Utilizes unlabeled data for pre-training and can be fine-tuned for different downstream tasks including cell-type and brain region classification.
                        </li>
                        <li>
                            NEMO outperforms current unsupervised (PhysMAP and VAEs) and supervised methods, with particularly strong performance in label-limited regimes.
                        </li>
                    </ul>
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Model schematic
                </h3>
                <img src="./index_files/images/NEMO_model_schematic.png" class="img-responsive" alt="method" style="width: 100%"><br>
                <p class="text-justify">
                    <i>NEMO utilizes a CLIP-based objective where an EAP encoder and an ACG image encoder are trained to embed randomly augmented EAPs and ACG image from the same neuron close together while keeping different neurons separate. The learned representations can be utilized for downstream tasks such as cell-type and brain-region classification. 
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Cerebellum cell-type classification (Beau et al. 2025)
                </h3>
                <img src="./index_files/images/c4_cerebellum_website.png" class="img-responsive" alt="cell-type ultra" style="width: 100%"><br>
                <h3>
                    Visual cortex cell-type classification (Ye et al. 2024)
                </h3>
                <img src="./index_files/images/np_ultra_website.png" class="img-responsive" alt="cell-type c4" style="width: 100%"><br>
                <p class="text-justify">
                    <i>Comparing NEMO to baseline models on two different optotagged datasets: an NP Ultra visual cortex (Ye et al. 2024) and a Neuropixels 1 cerebellum dataset (Beau et al. 2025). We show the UMAP visualization of NEMO representations for unseen opto-tagged units, colored by different cell-types. We also show the Balanced accuracy Confusion matrices normalized by ground truth label and averaged across 5 random seeds. NEMO outperforms the other embedding methods by a significant margin across all cell-types and evaluation methods.
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Brain region classification for the Brain-Wide Map (BWM; International Brain Laboratory et al. 2024)
                </h3>
                <img src="./index_files/images/region_classification_website.png" class="img-responsive" alt="region classification"><br>
                <p class="text-justify">
                    <i>Results for NEMO on the IBL brain region classification task. We show a schematic for multi-neuron classifier. At each depth, the neurons within 60 microns were used to classify the anatomical region. Only the nearest 5 neurons were selected if there were more than 5 neurons within that range. We average the logits of the single-neuron classifier (trained on NEMO embeddings) for all 5 neurons. The final prediction is based on the average of the individual logits.  We show confusion matrices for the single-neuron region classifier and multi-neuron region classifier for fine-tuned NEMO, averaged across 5 runs. We show the single neuron balanced accuracy with linear classifier and the MLP head for each model trained/fine-tuned with different label ratios. We also show the single-neuron MLP-classification balanced accuracy for each modality separately and for the combined representation.
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Clustering and visualization the NEMO representations for the BWM
                </h3>
                <img src="./index_files/images/region_clustering_website.png" class="img-responsive" alt="region clustering"><br>
                <p class="text-justify">
                    <i>IBL neuron clustering using NEMO. We show the UMAP visualization of the representations that NEMO extracts from the training data colored by anatomical brain region. We also show the same UMAP instead colored by cluster labels using a graph-based approach (Louvain clustering). We tuned the neighborhood size in UMAP and the resolution for the clustering. These parameters were selected by maximizing the modularity index which minimized the number of clusters. We show 2D brain slices across three brain views with the location of individual neurons colored using the cluster IDs. The black lines show the region boundaries of the Allen mouse atlas (Wang et al. 2020). The cluster distribution found using NEMO is closely correlated with the anatomical regions and is consistent across insertions from different labs.
            </div>
        </div>

        <div class="row" id="BibTeX">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
                If you find our data or project useful in your research, please cite:
                <pre class="w3-panel w3-leftbar w3-light-grey" style="white-space: pre-wrap; font-family: monospace; font-size: 10px">
 @inproceedings{
    yu2025in,
    title={In vivo cell-type and brain region classification via multimodal contrastive learning},
    author={Han Yu and Hanrui Lyu and YiXun Xu and Charlie Windolf and Eric Kenji Lee and Fan Yang and Andrew M Shelton and Olivier Winter and International Brain Laboratory and Eva L Dyer and Chandramouli Chandrasekaran and Nicholas A. Steinmetz and Liam Paninski and Cole Lincoln Hurwitz},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025},
    url={https://openreview.net/forum?id=10JOlFIPjt}
}</pre>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgments
                </h3>
                We thank Jonathan Pillow and Tatiana Engel for providing feedback on this manuscript. We also thank Maxime Beau and the other authors of Beau et al. 2025 for sharing the C4 cerebellum dataset. This project was supported by the Wellcome Trust (PRF 209558, 216324, 201225, and 224688 to MH, SHWF 221674 to LFR, collaborative award 204915 to MC, MH and TDH), National Institutes of Health (1U19NS123716), the Simons Foundation, the DoD OUSD (R\&E) under Cooperative Agreement PHY-2229929 (The NSF AI Institute for Artificial and Natural Intelligence), the Kavli Foundation, the Gatsby Charitable Foundation (GAT3708), the NIH BRAIN Initiative (U01NS113252 to NAS, SRO, and TDH), the Pew Biomedical Scholars Program (NAS), the Max Planck Society (GL), the European Research Council under the European Unionâ€™s Horizon 2020 research and innovation programme (grant agreement No 834446 to GL and AdG 695709 to MH), the Giovanni Armenise Harvard Foundation (CDA to LFR), the Human Technopole (ECF to LFR), the NSF (IOS 211500 to NBS), the Klingenstein-Simons Fellowship in Neuroscience (NAS), the NINDS R01NS122969, the NINDS R21NS135361, the NINDS F31NS131018, the NSF CAREER awards IIS-2146072, as well as generous gifts from the McKnight Foundation, and the CIFAR Azrieli Global Scholars Program. GM is supported by a Boehringer Ingelheim Fonds PhD Fellowship. The primate research procedures were supported by the NIH P51 (OD010425) to the WaNPRC, and animal breeding was supported by NIH U42 (OD011123). Computational modeling work was supported by the European Union Horizon 2020 Research and Innovation Programme under Grant Agreement No. 945539 Human Brain Project SGA3 and No. 101147319 EBRAINS 2.0 (GTE and TVN). Computational resources for building machine learning models were provided by ACCESS, which is funded by the US National Science Foundation.
                <p></p>
            </div>
        </div>
    </div>


</body></html>
