<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MB0YKTRBHZ"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-MB0YKTRBHZ');
    </script>
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>ibl-nemo</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="icon" type="image/png" href="./index_files/icon.png">
    <link rel="stylesheet" href="./index_files/bootstrap.min.css">
    <link rel="stylesheet" href="./index_files/font-awesome.min.css">
    <link rel="stylesheet" href="./index_files/codemirror.min.css">
    <link rel="stylesheet" href="./index_files/app.css">
    <link rel="stylesheet" href="./index_files/bootstrap.min(1).css">

    <script type="text/javascript" async="" src="./index_files/analytics.js"></script>
    <script type="text/javascript" async="" src="./index_files/analytics(1).js"></script>
    <script async="" src="./index_files/js"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-110862391-3');
    </script>

    <script src="./index_files/jquery.min.js"></script>
    <script src="./index_files/bootstrap.min.js"></script>
    <script src="./index_files/codemirror.min.js"></script>
    <script src="./index_files/clipboard.min.js"></script>

    <script src="./index_files/app.js"></script>
    <style>
        h1 {
            margin-bottom: 0px !important;
            padding-bottom: 0px !important;
        }
    </style>
</head>      
    
<body data-gr-c-s-loaded="true">
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                In vivo cell-type and brain region classification via multimodal contrastive learning
            <br /><br />
            <!-- <small>
                July 2024
            </small> -->
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="mailto:hy2562@columbia.edu">
                            Han Yu
                        </a><sup>1</sup>
                    </li>
                    <li>
                        <a href="mailto:hanruilyu2029@northwestern.edu">
                          Hanrui Lyu
                        </a><sup>2</sup>
                    </li>                    
                   <li>
                        <a href="mailto:yx2740@columbia.edu">
                            Ethan Yixun Xu
                        </a><sup>1</sup>
                    </li>
                    <li>
                        <a href="mailto:ciw2107@columbia.edu">
                            Charlie Windolf
                        </a><sup>1</sup>
                    </li> 
                    <li>
                        <a href="mailto:kenjilee@bu.edu">
                            Eric Kenji Lee
                        </a><sup>3</sup>
                    </li> 
                    <li>
                        <a href="mailto:yfan7809@gmail.com">
                            Fan Yang
                        </a><sup>4</sup>
                    </li> 
                    <li>
                        <a href="mailto:andrew.shelton1312@gmail.com">
                            Andrew M. Shelton
                        </a><sup>5</sup>
                    </li> 
                    <li>
                        <a href="https://alleninstitute.org/person/shawn-olsen/">
                            Shawn Olsen
                        </a><sup>5</sup>
                    </li> 
                    <li>
                        <a href="mailto:smanavi.ctp@gmail.com">
                            Sahar Minavi
                        </a><sup>5</sup>
                    </li> 
                    <li>
                        <a href="mailto:olivier.winter@internationalbrainlab.org">
                            Olivier Winter
                        </a><sup>6</sup>
                    </li> 
                    <li>
                        <a href="https://www.internationalbrainlab.com/">
                            The International Brain Laboratory
                        </a>
                    </li>   
                    <li>
                        <a href="https://dyerlab.gatech.edu/">
                            Eva L. Dyer
                        </a><sup>7</sup>
                    </li> 
                    <li>
                        <a href="https://sites.bu.edu/chandlab/">
                            Chandramouli Chandrasekaran
                        </a><sup>3</sup>
                    </li> 
                    <li>
                        <a href="https://www.nicksteinmetz.com/">
                            Nicholas A. Steinmetz
                        </a><sup>8</sup>
                    </li> 
                    <li>
                        <a href="http://www.stat.columbia.edu/~liam/">
                            Liam Paninski
                        </a><sup>1</sup>
                    </li>
                    <li>
                        <a href="https://colehurwitz.github.io/">
                            Cole Hurwitz
                        </a><sup>1</sup>
                    </li>                       
                </ul>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <sup>1</sup>Columbia University
                    </li>
                    <li>
                        <sup>2</sup>Northwestern University
                    </li>
                    <li>
                        <sup>3</sup>Boston University
                    </li>
                    <li>
                        <sup>4</sup>University of College London
                    </li>
                    <br>
                    <li>
                        <sup>5</sup>Allen Institute
                    </li>
                    <li>
                        <sup>6</sup>Champalimaud Foundation
                    </li>
                    <li>
                        <sup>7</sup>Georgia Institute of Technology
                    </li>
                    <li>
                        <sup>8</sup>University of Washington
                    </li>
                    <br>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://openreview.net/forum?id=10JOlFIPjt" target='_blank'>
                        <img src="index_files/images/nemo_paper_preview.png" height="80px"><br>
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/Haansololfp/NEMO">
                        <img src="./index_files/images/github_pad.png" height="80px"><br>
                            <h4><strong>Code</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="#BibTeX">
                        <img src="./index_files/images/bibtex.jpg" height="80px"><br>
                            <h4><strong>BibTeX</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
                <img src="./index_files/images/NEMO_overview.png" class="img-responsive" alt="overview"><br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Current electrophysiological approaches can track the activity of many neurons, yet it is usually unknown which cell-types or brain areas are being recorded without further molecular or histological analysis. Developing accurate and scalable algorithms for identifying the cell-type and brain region of recorded neurons is thus crucial for improving our understanding of neural computation. In this work, we develop a multimodal contrastive learning approach for neural data that can be fine-tuned for different downstream tasks, including inference of cell-type and brain location. We utilize multimodal contrastive learning to jointly embed the activity autocorrelations and extracellular waveforms of individual neurons. We demonstrate that our embedding approach, Neuronal Embeddings via MultimOdal Contrastive Learning (NEMO), paired with supervised fine-tuning, achieves state-of-the-art cell-type classification for two opto-tagged datasets and brain region classification for the public International Brain Laboratory Brain-wide Map dataset. Our method represents a promising step towards accurate cell-type and brain region classification from electrophysiological recordings. Code is availabe at https://github.com/Haansololfp/NEMO.
                </p>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Highlights
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            A multimodal contrastive learning method for electrophysiological data, Neuronal Embeddings via MultimOdal Contrastive Learning (NEMO).
                        </li>
                        <li>
                            Utilizes unlabeled data for pre-training and can be fine-tuned for different downstream tasks including cell-type and brain region classification.
                        </li>
                        <li>
                            NEMO outperforms current unsupervised (PhysMAP and VAEs) and supervised methods, with particularly strong performance in label-limited regimes.
                        </li>
                    </ul>
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Model schematic
                </h3>
                <img src="./index_files/images/NEMO_model_schematic.png" class="img-responsive" alt="method" style="width: 100%"><br>
                <p class="text-justify">
                    <i>NEMO utilizes a CLIP-based objective where an EAP encoder and an ACG image encoder are trained to embed randomly augmented EAPs and ACG image from the same neuron close together while keeping different neurons separate. The learned representations can be utilized for downstream tasks such as cell-type and brain-region classification. 
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Cerebellum cell-type classification
                </h3>
                <img src="./index_files/images/c4_cerebellum_website.png" class="img-responsive" alt="cell-type" style="width: 100%"><br>
                <h3>
                    Visual cortex cell-type classification
                </h3>
                <img src="./index_files/images/np_ultra_website.png" class="img-responsive" alt="cell-type" style="width: 100%"><br>
                <p class="text-justify">
                    <i>Comparing NEMO to baseline models on two different optotagged datasets: an NP Ultra visual cortex (Ye et al. 2024) and a Neuropixels 1 cerebellum dataset (Beau et al. 2025). We show the UMAP visualization of NEMO representations for unseen opto-tagged units, colored by different cell-types. We also show the Balanced accuracy Confusion matrices normalized by ground truth label and averaged across 5 random seeds. NEMO outperforms the other embedding methods by a significant margin across all cell-types and evaluation methods.
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Scale analysis
                </h3>
                <img src="./index_files/images/figure_4.png" class="img-responsive" alt="clustering analysis"><br>
                <p class="text-justify">
                    <i>Comparison of scaling curves between  NDT1-stitch pretrained with the MtM method vs. the temporal masking baseline.</i> The reported metrics - neuron-averaged bits per spike (bps), choice decoding accuracy, and whisker motion energy decoding R<sup>2</sup> - are averaged over all 5 held-out sessions. We fine-tune each pretrained model with its self-supervised loss (MtM or temporal) on the 5-heldout sessions and then evaluate with all of our metrics. "Num of Sessions" denotes the number of sessions used for pretraining.
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Behavior decoding from individual brain regions
                </h3>
                <img src="./index_files/images/figure_5.png" class="img-responsive" alt="zero-shot"><br>
                <p class="text-justify">
                    <i>Comparison of NDT1-stitch pretrained with the MtM method vs. the baseline temporal masking on behavior decoding from individual brain regions.</i> The rows display choice decoding accuracy and whisker motion energy decoding R<sup>2</sup>. Columns represent individual held-out sessions. Each point shows the behavior decoding performance when using neural activity from a specific brain region, with colors denoting different brain regions.
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Single neuron evaluation
                </h3>
                <img src="./index_files/images/figure_6.png" class="img-responsive" alt="zero-shot"><br>
                <p class="text-justify">
                    <b>Single neuron activity reconstruction analysis for NDT1 in one session.</b> To evaluate the reconstruction quality for each  neuron, multiple metrics are computed: Bits per spike (Bps), R<sup>2</sup> between the ground truth and predicted peristimulus time histogram (PSTH R<sup>2</sup>), and the single-trial R<sup>2</sup> averaged across all trials (Trial average R<sup>2</sup>). Each point represents one neuron, with the color indicating the neuron's log firing rates in Hertz (Hz).
            </div>
        </div>

        <div class="row" id="BibTeX">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
                If you find our data or project useful in your research, please cite:
                <pre class="w3-panel w3-leftbar w3-light-grey" style="white-space: pre-wrap; font-family: monospace; font-size: 10px">
@InProceedings{Zhang_2024_arXiv,
    author    = {Zhang, Yizi and Wang, Yanchen and Benetó, Donato Jiménez and Wang, Zixuan and Azabou, Mehdi and Richards, Blake and Winter, Olivier and The International Brain Laboratory and Dyer, Eva and Paninski, Liam and Hurwitz, Cole},
    title     = {Towards a “universal translator” for neural dynamics at single-cell, single-spike resolution},
    booktitle = {arXiv},
    month     = {July},
    year      = {2024},
    url       = {http://arxiv.org/abs/2407.14668}
}</pre>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgments
                </h3>
                This project was supported by the Wellcome Trust (209558 and 216324), National Institutes of Health (1U19NS123716), the Simons Foundation, the National Science Foundation (NSF award CIF:RI:2212182, NSF CAREER awards IIS-2146072), NSERC (Discovery Grant: RGPIN-2020-05105; Discovery Accelerator Supplement: RGPAS-2020-00031; Arthur B. McDonald Fellowship: 566355-2022) and CIFAR (Canada AI Chair; Learning in Machine and Brains Fellowship), and by DoD OUSD (R\&E) under Cooperative Agreement PHY-2229929 (The NSF AI Institute for Artificial and Natural Intelligence), as well as generous gifts from the Alfred Sloan Foundation, the McKnight Foundation, and the CIFAR Azrieli Global Scholars Program. The website template was borrowed from <a href="https://curvenet.github.io">here</a>.
                <p></p>
            </div>
        </div>
    </div>


</body></html>
